{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 RAG Agent with `inference-matcher-beeai`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beeai-framework==0.1.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from -r ../requirements.txt (line 1)) (0.1.1)\n",
      "Requirement already satisfied: pydantic==2.10.6 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from -r ../requirements.txt (line 2)) (2.10.6)\n",
      "Requirement already satisfied: langchain-community==0.3.18 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from -r ../requirements.txt (line 3)) (0.3.18)\n",
      "Requirement already satisfied: ipykernel in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from -r ../requirements.txt (line 4)) (6.29.5)\n",
      "Requirement already satisfied: fastapi==0.115.8 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from -r ../requirements.txt (line 5)) (0.115.8)\n",
      "Requirement already satisfied: Flask==3.1.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from -r ../requirements.txt (line 6)) (3.1.0)\n",
      "Requirement already satisfied: aiofiles<25.0.0,>=24.1.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (24.1.0)\n",
      "Requirement already satisfied: chevron<0.15.0,>=0.14.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: duckduckgo-search<8.0.0,>=7.3.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (7.5.5)\n",
      "Requirement already satisfied: json-repair<0.40.0,>=0.39.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.39.1)\n",
      "Requirement already satisfied: litellm<2.0.0,>=1.60.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (1.65.4.post1)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.2.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0,>=2.7 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: pylint<4.0.0,>=3.3.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (3.3.6)\n",
      "Requirement already satisfied: pyventus<0.7.0,>=0.6.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.32 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.32.0.20241016 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (2.32.0.20250328)\n",
      "Requirement already satisfied: wikipedia<2.0.0,>=1.4.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from pydantic==2.10.6->-r ../requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from pydantic==2.10.6->-r ../requirements.txt (line 2)) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from pydantic==2.10.6->-r ../requirements.txt (line 2)) (4.13.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.3.51)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.3.23)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-community==0.3.18->-r ../requirements.txt (line 3)) (2.0.40)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-community==0.3.18->-r ../requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-community==0.3.18->-r ../requirements.txt (line 3)) (3.11.16)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-community==0.3.18->-r ../requirements.txt (line 3)) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.6.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.3.24)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.4.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-community==0.3.18->-r ../requirements.txt (line 3)) (2.2.4)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from fastapi==0.115.8->-r ../requirements.txt (line 5)) (0.45.3)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from Flask==3.1.0->-r ../requirements.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from Flask==3.1.0->-r ../requirements.txt (line 6)) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from Flask==3.1.0->-r ../requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from Flask==3.1.0->-r ../requirements.txt (line 6)) (8.1.8)\n",
      "Requirement already satisfied: blinker>=1.9 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from Flask==3.1.0->-r ../requirements.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: comm>=0.1.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (1.8.13)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (9.0.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (1.6.0)\n",
      "Requirement already satisfied: packaging in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: psutil in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (26.4.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipykernel->-r ../requirements.txt (line 4)) (5.14.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (1.19.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: primp>=0.14.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from duckduckgo-search<8.0.0,>=7.3.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: lxml>=5.3.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from duckduckgo-search<8.0.0,>=7.3.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (5.3.2)\n",
      "Requirement already satisfied: decorator in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (3.0.50)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (0.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from Jinja2>=3.1.2->Flask==3.1.0->-r ../requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jupyter-client>=6.1.12->ipykernel->-r ../requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->-r ../requirements.txt (line 4)) (4.3.7)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain<1.0.0,>=0.3.19->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.3.8)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.37->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (3.10.16)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.23.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (8.6.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.68.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (1.70.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: tokenizers in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.21.1)\n",
      "Requirement already satisfied: anyio>=4.5 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.2.0->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.2.0->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.2.0->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.34.0)\n",
      "Requirement already satisfied: astroid<=3.4.0.dev0,>=3.3.8 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from pylint<4.0.0,>=3.3.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (3.3.9)\n",
      "Requirement already satisfied: dill>=0.3.6 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from pylint<4.0.0,>=3.3.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.3.9)\n",
      "Requirement already satisfied: isort!=5.13,<7,>=4.2.5 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from pylint<4.0.0,>=3.3.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: mccabe<0.8,>=0.6 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from pylint<4.0.0,>=3.3.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: tomlkit>=0.10.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from pylint<4.0.0,>=3.3.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from requests<3.0,>=2.32->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from requests<3.0,>=2.32->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from requests<3.0,>=2.32->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from requests<3.0,>=2.32->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (2025.1.31)\n",
      "Requirement already satisfied: greenlet>=1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (3.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from wikipedia<2.0.0,>=1.4.0->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (4.13.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.2.0->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (3.21.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (0.8.4)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.24.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from openai>=1.68.2->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from openai>=1.68.2->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: tqdm>4 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from openai>=1.68.2->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->-r ../requirements.txt (line 4)) (1.17.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from tiktoken>=0.7.0->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (2024.11.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.3.18->-r ../requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from beautifulsoup4->wikipedia<2.0.0,>=1.4.0->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel->-r ../requirements.txt (line 4)) (0.2.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from tokenizers->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (0.30.1)\n",
      "Requirement already satisfied: filelock in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm<2.0.0,>=1.60.2->beeai-framework==0.1.1->-r ../requirements.txt (line 1)) (2025.3.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting streamlit\n",
      "  Using cached streamlit-1.44.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting absl-py (from rouge-score)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: numpy in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from rouge-score) (2.2.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: click in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Collecting joblib (from nltk)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from streamlit) (1.9.0)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: packaging<25,>=20 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from streamlit) (24.2)\n",
      "Collecting pillow<12,>=7.1.0 (from streamlit)\n",
      "  Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting protobuf<6,>=3.20 (from streamlit)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from streamlit) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from streamlit) (9.1.2)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from streamlit) (4.13.1)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Downloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from streamlit) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.33.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Downloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading streamlit-1.44.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.2/731.2 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading pillow-11.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "Downloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Downloading narwhals-1.33.0-py3-none-any.whl (322 kB)\n",
      "Downloading smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24986 sha256=2056d53cd2adc6e1aad9a898197cd9801d7cce1f119106d940a51ed0d225a2aa\n",
      "  Stored in directory: /home/ruslanmv/.cache/pip/wheels/85/9d/af/01feefbe7d55ef5468796f0c68225b6788e85d9d0a281e7a70\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: pytz, watchdog, tzdata, toml, smmap, pyarrow, protobuf, pillow, narwhals, joblib, cachetools, absl-py, pydeck, pandas, nltk, gitdb, rouge-score, gitpython, altair, streamlit\n",
      "Successfully installed absl-py-2.2.2 altair-5.5.0 cachetools-5.5.2 gitdb-4.0.12 gitpython-3.1.44 joblib-1.4.2 narwhals-1.33.0 nltk-3.9.1 pandas-2.2.3 pillow-11.1.0 protobuf-5.29.4 pyarrow-19.0.1 pydeck-0.9.1 pytz-2025.2 rouge-score-0.1.2 smmap-5.0.2 streamlit-1.44.1 toml-0.10.2 tzdata-2025.2 watchdog-6.0.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##  Full Notebook Example of Usage\n",
    "# -----------------------------------------------------------\n",
    "# 🔧 Setup\n",
    "# -----------------------------------------------------------\n",
    "# Install all required packages. Run this cell only once.\n",
    "#!pip install -r ../requirements.txt\n",
    "#!pip install rouge-score nltk pandas streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🧪 Step 1: Data Cleaning\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we can import from parent src/ directory\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# Load raw dataset\n",
    "df = pd.read_csv(\"./data/rag_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ki_topic</th>\n",
       "      <th>ki_text</th>\n",
       "      <th>sample_question</th>\n",
       "      <th>sample_ground_truth</th>\n",
       "      <th>sample_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Setting Up a Mobile Device for Company Email</td>\n",
       "      <td>**Setting Up a Mobile Device for Company Email...</td>\n",
       "      <td>\"How do I set up my company email on my mobile...</td>\n",
       "      <td>To set up your company email on your mobile de...</td>\n",
       "      <td>To set up your company email on your mobile d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Resetting a Forgotten PIN</td>\n",
       "      <td>**Resetting a Forgotten PIN**\\n\\nIf you have f...</td>\n",
       "      <td>I forgot my PIN, how can I reset it?</td>\n",
       "      <td>Don't worry, I'm here to help To reset your fo...</td>\n",
       "      <td>To reset your forgotten PIN, follow these ste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Configuring VPN Access for Remote Workers</td>\n",
       "      <td>**Configuring VPN Access for Remote Workers**\\...</td>\n",
       "      <td>How do I set up VPN access on my laptop so I c...</td>\n",
       "      <td>To set up VPN access on your laptop and access...</td>\n",
       "      <td>To set up VPN access on your laptop, follow t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Troubleshooting Issues with Microsoft Office</td>\n",
       "      <td>**Troubleshooting Issues with Microsoft Office...</td>\n",
       "      <td>\"My Microsoft Word keeps freezing every time I...</td>\n",
       "      <td>I'd be happy to help you troubleshoot the issu...</td>\n",
       "      <td>Try restarting Microsoft Word, then check for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Setting Up a Conference Call on Cisco Webex</td>\n",
       "      <td>To set up a conference call on Cisco Webex, fo...</td>\n",
       "      <td>How do I set up a conference call on Cisco Web...</td>\n",
       "      <td>To set up a conference call on Cisco Webex wit...</td>\n",
       "      <td>To set up a conference call on Cisco Webex wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Creating a Backup of Important Files</td>\n",
       "      <td>**Creating a Backup of Important Files**\\n\\nBa...</td>\n",
       "      <td>\"How do I back up my important work files to p...</td>\n",
       "      <td>To back up your important work files, follow t...</td>\n",
       "      <td>To back up your important work files, follow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Troubleshooting Issues with Company-Issued Tab...</td>\n",
       "      <td>**Troubleshooting Issues with Company-Issued T...</td>\n",
       "      <td>\"My company-issued tablet is freezing frequent...</td>\n",
       "      <td>I'd be happy to help you troubleshoot the issu...</td>\n",
       "      <td>To fix the issue with your company-issued tab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Setting Up a Secure Wireless Network</td>\n",
       "      <td>**Step 1: Plan Your Wireless Network**\\n\\nBefo...</td>\n",
       "      <td>\"How do I set up a secure wireless network for...</td>\n",
       "      <td>To set up a secure wireless network for your w...</td>\n",
       "      <td>To set up a secure wireless network, follow t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Resetting a Jammed Printer</td>\n",
       "      <td>**Resetting a Jammed Printer**\\n\\n**Step 1: Tu...</td>\n",
       "      <td>What steps can I take to fix my printer when i...</td>\n",
       "      <td>Don't worry, I'm here to help To fix a jammed ...</td>\n",
       "      <td>To fix a jammed printer, follow these steps: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Configuring Email on an Android Device</td>\n",
       "      <td>**Configuring Email on an Android Device**\\n\\n...</td>\n",
       "      <td>How do I set up my company email on my persona...</td>\n",
       "      <td>To set up your company email on your personal ...</td>\n",
       "      <td>To set up your company email on your personal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           ki_topic  \\\n",
       "0           0       Setting Up a Mobile Device for Company Email   \n",
       "1           1                          Resetting a Forgotten PIN   \n",
       "2           2          Configuring VPN Access for Remote Workers   \n",
       "3           3       Troubleshooting Issues with Microsoft Office   \n",
       "4           4        Setting Up a Conference Call on Cisco Webex   \n",
       "5           5               Creating a Backup of Important Files   \n",
       "6           6  Troubleshooting Issues with Company-Issued Tab...   \n",
       "7           7               Setting Up a Secure Wireless Network   \n",
       "8           8                         Resetting a Jammed Printer   \n",
       "9           9             Configuring Email on an Android Device   \n",
       "\n",
       "                                             ki_text  \\\n",
       "0  **Setting Up a Mobile Device for Company Email...   \n",
       "1  **Resetting a Forgotten PIN**\\n\\nIf you have f...   \n",
       "2  **Configuring VPN Access for Remote Workers**\\...   \n",
       "3  **Troubleshooting Issues with Microsoft Office...   \n",
       "4  To set up a conference call on Cisco Webex, fo...   \n",
       "5  **Creating a Backup of Important Files**\\n\\nBa...   \n",
       "6  **Troubleshooting Issues with Company-Issued T...   \n",
       "7  **Step 1: Plan Your Wireless Network**\\n\\nBefo...   \n",
       "8  **Resetting a Jammed Printer**\\n\\n**Step 1: Tu...   \n",
       "9  **Configuring Email on an Android Device**\\n\\n...   \n",
       "\n",
       "                                     sample_question  \\\n",
       "0  \"How do I set up my company email on my mobile...   \n",
       "1               I forgot my PIN, how can I reset it?   \n",
       "2  How do I set up VPN access on my laptop so I c...   \n",
       "3  \"My Microsoft Word keeps freezing every time I...   \n",
       "4  How do I set up a conference call on Cisco Web...   \n",
       "5  \"How do I back up my important work files to p...   \n",
       "6  \"My company-issued tablet is freezing frequent...   \n",
       "7  \"How do I set up a secure wireless network for...   \n",
       "8  What steps can I take to fix my printer when i...   \n",
       "9  How do I set up my company email on my persona...   \n",
       "\n",
       "                                 sample_ground_truth  \\\n",
       "0  To set up your company email on your mobile de...   \n",
       "1  Don't worry, I'm here to help To reset your fo...   \n",
       "2  To set up VPN access on your laptop and access...   \n",
       "3  I'd be happy to help you troubleshoot the issu...   \n",
       "4  To set up a conference call on Cisco Webex wit...   \n",
       "5  To back up your important work files, follow t...   \n",
       "6  I'd be happy to help you troubleshoot the issu...   \n",
       "7  To set up a secure wireless network for your w...   \n",
       "8  Don't worry, I'm here to help To fix a jammed ...   \n",
       "9  To set up your company email on your personal ...   \n",
       "\n",
       "                                       sample_answer  \n",
       "0   To set up your company email on your mobile d...  \n",
       "1   To reset your forgotten PIN, follow these ste...  \n",
       "2   To set up VPN access on your laptop, follow t...  \n",
       "3   Try restarting Microsoft Word, then check for...  \n",
       "4   To set up a conference call on Cisco Webex wi...  \n",
       "5   To back up your important work files, follow ...  \n",
       "6   To fix the issue with your company-issued tab...  \n",
       "7   To set up a secure wireless network, follow t...  \n",
       "8   To fix a jammed printer, follow these steps: ...  \n",
       "9   To set up your company email on your personal...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)                            # remove URLs\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)                       # remove alphanumerics\n",
    "    text = re.sub(r'#\\S+', '', text)                               # remove hashtags\n",
    "    text = re.sub(r'<.*?>', '', text)                              # remove HTML tags\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ki_topic</th>\n",
       "      <th>ki_text</th>\n",
       "      <th>sample_question</th>\n",
       "      <th>sample_ground_truth</th>\n",
       "      <th>sample_answer</th>\n",
       "      <th>cleaned_ki_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Setting Up a Mobile Device for Company Email</td>\n",
       "      <td>**Setting Up a Mobile Device for Company Email...</td>\n",
       "      <td>\"How do I set up my company email on my mobile...</td>\n",
       "      <td>To set up your company email on your mobile de...</td>\n",
       "      <td>To set up your company email on your mobile d...</td>\n",
       "      <td>**Setting Up a Mobile Device for Company Email...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Resetting a Forgotten PIN</td>\n",
       "      <td>**Resetting a Forgotten PIN**\\n\\nIf you have f...</td>\n",
       "      <td>I forgot my PIN, how can I reset it?</td>\n",
       "      <td>Don't worry, I'm here to help To reset your fo...</td>\n",
       "      <td>To reset your forgotten PIN, follow these ste...</td>\n",
       "      <td>**Resetting a Forgotten PIN**\\n\\nIf you have f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Configuring VPN Access for Remote Workers</td>\n",
       "      <td>**Configuring VPN Access for Remote Workers**\\...</td>\n",
       "      <td>How do I set up VPN access on my laptop so I c...</td>\n",
       "      <td>To set up VPN access on your laptop and access...</td>\n",
       "      <td>To set up VPN access on your laptop, follow t...</td>\n",
       "      <td>**Configuring VPN Access for Remote Workers**\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Troubleshooting Issues with Microsoft Office</td>\n",
       "      <td>**Troubleshooting Issues with Microsoft Office...</td>\n",
       "      <td>\"My Microsoft Word keeps freezing every time I...</td>\n",
       "      <td>I'd be happy to help you troubleshoot the issu...</td>\n",
       "      <td>Try restarting Microsoft Word, then check for...</td>\n",
       "      <td>**Troubleshooting Issues with Microsoft Office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Setting Up a Conference Call on Cisco Webex</td>\n",
       "      <td>To set up a conference call on Cisco Webex, fo...</td>\n",
       "      <td>How do I set up a conference call on Cisco Web...</td>\n",
       "      <td>To set up a conference call on Cisco Webex wit...</td>\n",
       "      <td>To set up a conference call on Cisco Webex wi...</td>\n",
       "      <td>To set up a conference call on Cisco Webex, fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      ki_topic  \\\n",
       "0           0  Setting Up a Mobile Device for Company Email   \n",
       "1           1                     Resetting a Forgotten PIN   \n",
       "2           2     Configuring VPN Access for Remote Workers   \n",
       "3           3  Troubleshooting Issues with Microsoft Office   \n",
       "4           4   Setting Up a Conference Call on Cisco Webex   \n",
       "\n",
       "                                             ki_text  \\\n",
       "0  **Setting Up a Mobile Device for Company Email...   \n",
       "1  **Resetting a Forgotten PIN**\\n\\nIf you have f...   \n",
       "2  **Configuring VPN Access for Remote Workers**\\...   \n",
       "3  **Troubleshooting Issues with Microsoft Office...   \n",
       "4  To set up a conference call on Cisco Webex, fo...   \n",
       "\n",
       "                                     sample_question  \\\n",
       "0  \"How do I set up my company email on my mobile...   \n",
       "1               I forgot my PIN, how can I reset it?   \n",
       "2  How do I set up VPN access on my laptop so I c...   \n",
       "3  \"My Microsoft Word keeps freezing every time I...   \n",
       "4  How do I set up a conference call on Cisco Web...   \n",
       "\n",
       "                                 sample_ground_truth  \\\n",
       "0  To set up your company email on your mobile de...   \n",
       "1  Don't worry, I'm here to help To reset your fo...   \n",
       "2  To set up VPN access on your laptop and access...   \n",
       "3  I'd be happy to help you troubleshoot the issu...   \n",
       "4  To set up a conference call on Cisco Webex wit...   \n",
       "\n",
       "                                       sample_answer  \\\n",
       "0   To set up your company email on your mobile d...   \n",
       "1   To reset your forgotten PIN, follow these ste...   \n",
       "2   To set up VPN access on your laptop, follow t...   \n",
       "3   Try restarting Microsoft Word, then check for...   \n",
       "4   To set up a conference call on Cisco Webex wi...   \n",
       "\n",
       "                                     cleaned_ki_text  \n",
       "0  **Setting Up a Mobile Device for Company Email...  \n",
       "1  **Resetting a Forgotten PIN**\\n\\nIf you have f...  \n",
       "2  **Configuring VPN Access for Remote Workers**\\...  \n",
       "3  **Troubleshooting Issues with Microsoft Office...  \n",
       "4  To set up a conference call on Cisco Webex, fo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply cleaning\n",
    "df[\"cleaned_ki_text\"] = df[\"ki_text\"].apply(clean_text)\n",
    "df.to_csv(\"./data/rag_cleaned.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 📥 Step 2: Load Prompt Templates & Setup Agent Inputs\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "from src.templates.agent_generation_template import agent_generation_template\n",
    "from src.evaluation.evaluation_utils import calculate_similarity, meets_criteria\n",
    "from src.workflows.inference_matcher_workflow import inference_workflow, InferenceMatcherState\n",
    "import json\n",
    "import asyncio\n",
    "# Join cleaned content into one string for prompt base\n",
    "document_template = \"\\n\".join(df[\"cleaned_ki_text\"].tolist())\n",
    "# Search parameters for different model or method settings\n",
    "parameters = [\n",
    "    \"use_embedding=True, model=watsonx/granite-8b\",\n",
    "    \"use_embedding=False, model=watsonx/codellama\",\n",
    "    \"retriever_method=cosine\"\n",
    "]\n",
    "\n",
    "# Define what our target agent should produce\n",
    "target_config = {\n",
    "    \"target_description\": \"The generated inference should represent a valid configuration for a RAG support agent.\",\n",
    "    \"criteria\": {\n",
    "        \"context_usage\": \"true\",\n",
    "        \"conciseness\": \"high\",\n",
    "        \"retrieval_citation\": \"true\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🚀 Step 3: Run Inference Matcher Workflow\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Initialize agent state for BeeAI workflow\n",
    "state = InferenceMatcherState(\n",
    "    document_template=document_template,\n",
    "    target_output=target_config,\n",
    "    search_parameters=parameters,\n",
    "    max_iterations=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.workflows.inference_matcher_workflow:[Step: Generate] Using Parameters: use_embedding=True, model=watsonx/granite-8b\n",
      "\u001b[92m13:16:49 - LiteLLM:INFO\u001b[0m: utils.py:3075 - \n",
      "LiteLLM completion() model= llama3.1; provider = ollama_chat\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= llama3.1; provider = ollama_chat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py:1115\u001b[39m, in \u001b[36mTCPConnector._wrap_create_connection\u001b[39m\u001b[34m(self, addr_infos, req, timeout, client_error, *args, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ceil_timeout(\n\u001b[32m   1113\u001b[39m     timeout.sock_connect, ceil_threshold=timeout.ceil_threshold\n\u001b[32m   1114\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1115\u001b[39m     sock = \u001b[38;5;28;01mawait\u001b[39;00m aiohappyeyeballs.start_connection(\n\u001b[32m   1116\u001b[39m         addr_infos=addr_infos,\n\u001b[32m   1117\u001b[39m         local_addr_infos=\u001b[38;5;28mself\u001b[39m._local_addr_infos,\n\u001b[32m   1118\u001b[39m         happy_eyeballs_delay=\u001b[38;5;28mself\u001b[39m._happy_eyeballs_delay,\n\u001b[32m   1119\u001b[39m         interleave=\u001b[38;5;28mself\u001b[39m._interleave,\n\u001b[32m   1120\u001b[39m         loop=\u001b[38;5;28mself\u001b[39m._loop,\n\u001b[32m   1121\u001b[39m     )\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._loop.create_connection(*args, **kwargs, sock=sock)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohappyeyeballs/impl.py:122\u001b[39m, in \u001b[36mstart_connection\u001b[39m\u001b[34m(addr_infos, local_addr_infos, happy_eyeballs_delay, interleave, loop, socket_factory)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_exceptions) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m first_exception\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m# If they all have the same str(), raise one.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohappyeyeballs/impl.py:73\u001b[39m, in \u001b[36mstart_connection\u001b[39m\u001b[34m(addr_infos, local_addr_infos, happy_eyeballs_delay, interleave, loop, socket_factory)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     sock = \u001b[38;5;28;01mawait\u001b[39;00m _connect_sock(\n\u001b[32m     74\u001b[39m         current_loop,\n\u001b[32m     75\u001b[39m         exceptions,\n\u001b[32m     76\u001b[39m         addrinfo,\n\u001b[32m     77\u001b[39m         local_addr_infos,\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     79\u001b[39m         socket_factory,\n\u001b[32m     80\u001b[39m     )\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohappyeyeballs/impl.py:208\u001b[39m, in \u001b[36m_connect_sock\u001b[39m\u001b[34m(loop, exceptions, addr_info, local_addr_infos, open_sockets, socket_factory)\u001b[39m\n\u001b[32m    207\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mno matching local address with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfamily\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m loop.sock_connect(sock, address)\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sock\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/selector_events.py:651\u001b[39m, in \u001b[36mBaseSelectorEventLoop.sock_connect\u001b[39m\u001b[34m(self, sock, address)\u001b[39m\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    653\u001b[39m     \u001b[38;5;66;03m# Needed to break cycles when an exception occurs.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/futures.py:289\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m     \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m     \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/selector_events.py:691\u001b[39m, in \u001b[36mBaseSelectorEventLoop._sock_connect_cb\u001b[39m\u001b[34m(self, fut, sock, address)\u001b[39m\n\u001b[32m    689\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m err != \u001b[32m0\u001b[39m:\n\u001b[32m    690\u001b[39m         \u001b[38;5;66;03m# Jump to any except clause below.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m691\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(err, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mConnect call failed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maddress\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    692\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mBlockingIOError\u001b[39;00m, \u001b[38;5;167;01mInterruptedError\u001b[39;00m):\n\u001b[32m    693\u001b[39m     \u001b[38;5;66;03m# socket is still registered, the callback will be retried later\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 111] Connect call failed ('127.0.0.1', 11434)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mClientConnectorError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/main.py:477\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/llms/ollama_chat.py:607\u001b[39m, in \u001b[36mollama_acompletion\u001b[39m\u001b[34m(url, api_key, data, model_response, encoding, logging_obj, function_name)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/llms/ollama_chat.py:542\u001b[39m, in \u001b[36mollama_acompletion\u001b[39m\u001b[34m(url, api_key, data, model_response, encoding, logging_obj, function_name)\u001b[39m\n\u001b[32m    541\u001b[39m     _request[\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mBearer \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(api_key)}\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m resp = \u001b[38;5;28;01mawait\u001b[39;00m session.post(**_request)\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m resp.status != \u001b[32m200\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/client.py:703\u001b[39m, in \u001b[36mClientSession._request\u001b[39m\u001b[34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m     conn = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connector.connect(\n\u001b[32m    704\u001b[39m         req, traces=traces, timeout=real_timeout\n\u001b[32m    705\u001b[39m     )\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.TimeoutError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py:548\u001b[39m, in \u001b[36mBaseConnector.connect\u001b[39m\u001b[34m(self, req, traces, timeout)\u001b[39m\n\u001b[32m    547\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m trace.send_connection_create_start()\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m proto = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_connection(req, traces, timeout)\n\u001b[32m    549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m traces:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py:1056\u001b[39m, in \u001b[36mTCPConnector._create_connection\u001b[39m\u001b[34m(self, req, traces, timeout)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     _, proto = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_direct_connection(req, traces, timeout)\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m proto\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py:1400\u001b[39m, in \u001b[36mTCPConnector._create_direct_connection\u001b[39m\u001b[34m(self, req, traces, timeout, client_error)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m last_exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1400\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m last_exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py:1369\u001b[39m, in \u001b[36mTCPConnector._create_direct_connection\u001b[39m\u001b[34m(self, req, traces, timeout, client_error)\u001b[39m\n\u001b[32m   1368\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m     transp, proto = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wrap_create_connection(\n\u001b[32m   1370\u001b[39m         \u001b[38;5;28mself\u001b[39m._factory,\n\u001b[32m   1371\u001b[39m         timeout=timeout,\n\u001b[32m   1372\u001b[39m         ssl=sslcontext,\n\u001b[32m   1373\u001b[39m         addr_infos=addr_infos,\n\u001b[32m   1374\u001b[39m         server_hostname=server_hostname,\n\u001b[32m   1375\u001b[39m         req=req,\n\u001b[32m   1376\u001b[39m         client_error=client_error,\n\u001b[32m   1377\u001b[39m     )\n\u001b[32m   1378\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ClientConnectorError, asyncio.TimeoutError) \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py:1130\u001b[39m, in \u001b[36mTCPConnector._wrap_create_connection\u001b[39m\u001b[34m(self, addr_infos, req, timeout, client_error, *args, **kwargs)\u001b[39m\n\u001b[32m   1129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1130\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m client_error(req.connection_key, exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mClientConnectorError\u001b[39m: Cannot connect to host localhost:11434 ssl:default [Connect call failed ('127.0.0.1', 11434)]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/beeai_framework/backend/chat.py:233\u001b[39m, in \u001b[36mChatModel.create.<locals>.run_create\u001b[39m\u001b[34m(context)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create(\u001b[38;5;28minput\u001b[39m, context)\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m context.emitter.emit(\u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m, {\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m: result})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/beeai_framework/adapters/litellm/chat.py:76\u001b[39m, in \u001b[36mLiteLLMChatModel._create\u001b[39m\u001b[34m(self, input, run)\u001b[39m\n\u001b[32m     75\u001b[39m litellm_input = \u001b[38;5;28mself\u001b[39m._transform_input(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m acompletion(**litellm_input.model_dump())\n\u001b[32m     77\u001b[39m response_message = response.get(\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m, [{}])[\u001b[32m0\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/utils.py:1452\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1451\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1452\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/utils.py:1313\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1312\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1313\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1314\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/main.py:496\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, base_url, api_version, api_key, model_list, extra_headers, thinking, **kwargs)\u001b[39m\n\u001b[32m    495\u001b[39m custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2213\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2214\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2190\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2189\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2190\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(\n\u001b[32m   2191\u001b[39m                 message=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2192\u001b[39m                     \u001b[38;5;28mstr\u001b[39m(original_exception), traceback.format_exc()\n\u001b[32m   2193\u001b[39m                 ),\n\u001b[32m   2194\u001b[39m                 llm_provider=custom_llm_provider,\n\u001b[32m   2195\u001b[39m                 model=model,\n\u001b[32m   2196\u001b[39m                 request=httpx.Request(\n\u001b[32m   2197\u001b[39m                     method=\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2198\u001b[39m                 ),  \u001b[38;5;66;03m# stub the request\u001b[39;00m\n\u001b[32m   2199\u001b[39m             )\n\u001b[32m   2200\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2201\u001b[39m     \u001b[38;5;66;03m# LOGGING\u001b[39;00m\n",
      "\u001b[31mAPIConnectionError\u001b[39m: litellm.APIConnectionError: Cannot connect to host localhost:11434 ssl:default [Connect call failed ('127.0.0.1', 11434)]\nTraceback (most recent call last):\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1115, in _wrap_create_connection\n    sock = await aiohappyeyeballs.start_connection(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 122, in start_connection\n    raise first_exception\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 73, in start_connection\n    sock = await _connect_sock(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohappyeyeballs/impl.py\", line 208, in _connect_sock\n    await loop.sock_connect(sock, address)\n  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 651, in sock_connect\n    return await fut\n           ^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/futures.py\", line 289, in __await__\n    yield self  # This tells Task to wait for completion.\n    ^^^^^^^^^^\n  File \"/usr/lib/python3.12/asyncio/tasks.py\", line 385, in __wakeup\n    future.result()\n  File \"/usr/lib/python3.12/asyncio/futures.py\", line 202, in result\n    raise self._exception.with_traceback(self._exception_tb)\n  File \"/usr/lib/python3.12/asyncio/selector_events.py\", line 691, in _sock_connect_cb\n    raise OSError(err, f'Connect call failed {address}')\nConnectionRefusedError: [Errno 111] Connect call failed ('127.0.0.1', 11434)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/main.py\", line 477, in acompletion\n    response = await init_response\n               ^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/llms/ollama_chat.py\", line 607, in ollama_acompletion\n    raise e  # don't use verbose_logger.exception, if exception is raised\n    ^^^^^^^\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/litellm/llms/ollama_chat.py\", line 542, in ollama_acompletion\n    resp = await session.post(**_request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/client.py\", line 703, in _request\n    conn = await self._connector.connect(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 548, in connect\n    proto = await self._create_connection(req, traces, timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1056, in _create_connection\n    _, proto = await self._create_direct_connection(req, traces, timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1400, in _create_direct_connection\n    raise last_exc\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1369, in _create_direct_connection\n    transp, proto = await self._wrap_create_connection(\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/aiohttp/connector.py\", line 1130, in _wrap_create_connection\n    raise client_error(req.connection_key, exc) from exc\naiohttp.client_exceptions.ClientConnectorError: Cannot connect to host localhost:11434 ssl:default [Connect call failed ('127.0.0.1', 11434)]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m nest_asyncio.apply()\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Run the async workflow properly\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m inference_workflow.run(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/beeai_framework/workflows/workflow.py:132\u001b[39m, in \u001b[36mWorkflow.run\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    129\u001b[39m run.steps.append(step_res)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inspect.iscoroutinefunction(step.handler):\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     step_next = \u001b[38;5;28;01mawait\u001b[39;00m step.handler(step_res.state)\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    134\u001b[39m     step_next = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.to_thread(step.handler, step_res.state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/src/workflows/inference_matcher_workflow.py:53\u001b[39m, in \u001b[36mgenerate_inference\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     50\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Rendered Prompt]:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrendered_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m message = UserMessage(content=rendered_prompt)\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m output = \u001b[38;5;28;01mawait\u001b[39;00m chat_model.create(ChatModelInput(messages=[message]))\n\u001b[32m     54\u001b[39m state.current_inference = output.get_text_content()\n\u001b[32m     56\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Generated Inference]:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstate.current_inference\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/beeai_framework/context.py:74\u001b[39m, in \u001b[36mRun._run_tasks\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m ensure_async(fn)(param)\n\u001b[32m     73\u001b[39m \u001b[38;5;28mself\u001b[39m.tasks.clear()\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handler()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/beeai_framework/context.py:161\u001b[39m, in \u001b[36mRunContext.enter.<locals>.handler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    159\u001b[39m result: R | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m first_done \u001b[38;5;129;01min\u001b[39;00m asyncio.as_completed([abort_task, runner_task]):\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m first_done\n\u001b[32m    162\u001b[39m     abort_task.cancel()\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/tasks.py:631\u001b[39m, in \u001b[36mas_completed.<locals>._wait_for_one\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# Dummy value from _on_timeout().\u001b[39;00m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TimeoutError\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/futures.py:202\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/beeai_framework/context.py:141\u001b[39m, in \u001b[36mRunContext.enter.<locals>.handler.<locals>._context_storage_run\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_context_storage_run\u001b[39m() -> R:\n\u001b[32m    140\u001b[39m     RunContext.storage.set(context)\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn(context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/beeai_framework/backend/chat.py:241\u001b[39m, in \u001b[36mChatModel.create.<locals>.run_create\u001b[39m\u001b[34m(context)\u001b[39m\n\u001b[32m    239\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m context.emitter.emit(\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m, {\u001b[38;5;28minput\u001b[39m, ex})\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatModelError(\u001b[33m\"\u001b[39m\u001b[33mModel error has occurred.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Github/Projects/best_inference_matcher_llm/.venv/lib/python3.12/site-packages/pydantic/_internal/_model_construction.py:511\u001b[39m, in \u001b[36mmake_hash_func.<locals>.hash_func\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhash_func\u001b[39m(\u001b[38;5;28mself\u001b[39m: Any) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgetter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# In rare cases (such as when using the deprecated copy method), the __dict__ may not contain\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# all model fields, which is how we can get here.\u001b[39;00m\n\u001b[32m    515\u001b[39m         \u001b[38;5;66;03m# getter(self.__dict__) is much faster than any 'safe' method that accounts for missing keys,\u001b[39;00m\n\u001b[32m    516\u001b[39m         \u001b[38;5;66;03m# and wrapping it in a `try` doesn't slow things down much in the common case.\u001b[39;00m\n\u001b[32m    517\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhash\u001b[39m(getter(SafeGetItemProxy(\u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m)))\n",
      "\u001b[31mTypeError\u001b[39m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import asyncio\n",
    "\n",
    "# Patch the existing running loop\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Run the async workflow properly\n",
    "result = await inference_workflow.run(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run async workflow\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43minference_workflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/asyncio/runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final result\n",
    "print(\"✅ Best Inference:\")\n",
    "print(result.state.best_inference)\n",
    "print(\"💡 Score:\", result.state.best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 📊 Step 4: Evaluation (ROUGE & Custom Relevance)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Initialize scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "scores = []\n",
    "\n",
    "# Evaluate each AI-generated answer against ground truth\n",
    "for i, row in df.iterrows():\n",
    "    rouge = scorer.score(row[\"sample_ground_truth\"], row[\"sample_answer\"])\n",
    "    rel = SequenceMatcher(None, row[\"sample_ground_truth\"], row[\"sample_answer\"]).ratio()\n",
    "    scores.append({\n",
    "        \"index\": i,\n",
    "        \"ROUGE-L\": round(rouge[\"rougeL\"].fmeasure, 4),\n",
    "        \"Relevance\": round(rel, 4)\n",
    "    })\n",
    "\n",
    "# Show results\n",
    "eval_df = pd.DataFrame(scores)\n",
    "eval_df.style.bar(subset=[\"ROUGE-L\", \"Relevance\"], color='#5fba7d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# 🔁 Optional: Launch the Streamlit Debugger from CLI\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# To use the debugging assistant with WatsonX, run from terminal:\n",
    "# !streamlit run ../src/debugger/debugger_app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
