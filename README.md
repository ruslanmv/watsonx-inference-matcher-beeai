
# ğŸ§  Iterative Inference Matcher with BeeAI Framework

Generalized Agent Generation through Multi-Round Inference and Evaluation using LLMs and BeeAI Workflows.

## ğŸ“Œ Project Overview

This project implements a robust **Iterative Inference Matcher** using the **BeeAI framework**, enhanced with:

- Multi-step agent generation through iterative LLM inference
- Automatic evaluation and comparison logic
- Error recovery and auto-retry using WatsonX AI Debugger
- Streamlit-based frontend wizard for guided agent creation

It is designed to **generalize agent generation** across diverse infrastructure templates and supports automated testing, correction, and optimization of code via AI feedback loops.



## âš™ï¸ Features

âœ… Iterative generation via BeeAI Workflows  
âœ… Prompt Templates for flexible prompt engineering  
âœ… Target-based evaluation scoring  
âœ… Auto-error fixing and retry using WatsonX  
âœ… Streamlit wizard UI to collect agent specs from users  
âœ… Full modular Python backend with extensibility  

---

## ğŸ“ Project Structure

```bash
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â””â”€â”€ inference_matcher_workflow.py      # BeeAI workflow engine
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”œâ”€â”€ inference_template.py              # Prompt template
â”‚   â”‚   â””â”€â”€ agent_generation_template.py       # Specialized for agent configs
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â””â”€â”€ evaluation_utils.py                # Custom scoring logic
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ targets.json                       # Target criteria definitions
â”‚   â”œâ”€â”€ examples/
â”‚   â”‚   â””â”€â”€ sample_document.txt                # Example input template
â”‚   â”œâ”€â”€ debugger/
â”‚   â”‚   â”œâ”€â”€ debugger_app.py                    # Streamlit app for error fixing
â”‚   â”‚   â””â”€â”€ utils.py                           # WatsonX integration logic
â”‚   â”œâ”€â”€ frontend/
â”‚   â”‚   â””â”€â”€ wizard_app.py                      # Web wizard for agent creation
â”‚   â””â”€â”€ tests/
â”‚       â”œâ”€â”€ test_inference_workflow.py         # Workflow test cases
â”‚       â””â”€â”€ test_evaluation.py                 # Evaluation tests
â”œâ”€â”€ .env                                       # API credentials (not committed)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ docs/
    â””â”€â”€ documentation.md                               # Architecture & design report
```



## ğŸ§ª Installation & Setup Guide

### âœ… 1. Clone the Repository

```bash
git clone https://github.com/ruslanmv/inference-matcher-beeai.git
cd inference-matcher-beeai
```

### âœ… 2. Create and Activate Virtual Environment

```bash
python -m venv venv
source venv/bin/activate    # macOS/Linux
venv\Scripts\activate       # Windows
```

### âœ… 3. Install Requirements

```bash
pip install -r requirements.txt
```

Make sure `beeai-framework`, `streamlit`, `pydantic`, `python-dotenv`, and `ibm-watson-machine-learning` are included.

### âœ… 4. Configure WatsonX

Create a `.env` file in root:

```
API_KEY=your_watsonx_api_key
PROJECT_ID=your_project_id
```



## ğŸš€ Running the Project

### ğŸŒ€ Run Inference Matcher (Backend)

```bash
python -m src.workflows.inference_matcher_workflow
```

Edit the `InferenceMatcherState` to customize:
- `document_template` â†’ Text prompt
- `target_output` â†’ Scoring goals from `config/targets.json`
- `search_parameters` â†’ List of parameters to iterate
- `max_iterations` â†’ Loop limit



### ğŸ” WatsonX Auto Debug Assistant

Run via Streamlit:

```bash
streamlit run src/debugger/debugger_app.py
```

Paste buggy code â†’ Auto suggestions â†’ Logs corrections



### ğŸ§™ Wizard UI for Agent Creation

Launch wizard with:

```bash
streamlit run src/frontend/wizard_app.py
```

Wizard will:
- Ask questions about the system
- Generate agent configs via BeeAI
- Return the best-matched inference



## ğŸ§ª Run Tests

```bash
pytest src/tests/
```

Validates:
- Inference loop
- Score comparisons
- Error recovery paths



## ğŸ“Œ Sample: `InferenceMatcherState`

```python
import json
import asyncio
from src.workflows.inference_matcher_workflow import inference_workflow, InferenceMatcherState

async def run():
    state = InferenceMatcherState(
        document_template=open("src/examples/sample_document.txt").read(),
        target_output=json.load(open("src/config/targets.json")),
        search_parameters=["version=1", "version=2", "version=3"],
        max_iterations=3
    )
    result = await inference_workflow.run(state)
    print("Best inference:", result.state.best_inference)

asyncio.run(run())
```



## ğŸ“š Documentation

See technical breakdown in:

```
docs/PAPER.md
```

Covers:
- Prompt templates
- BeeAI workflow architecture
- Evaluation logic
- Debug strategy



## ğŸ¯ Advanced Configuration

| Field               | Description                                  |
|--------------------|----------------------------------------------|
| `max_iterations`   | Number of inference attempts allowed         |
| `search_parameters`| Prompt variations or tuning inputs           |
| `target_output`    | Goal structure (metrics, fields)             |
| `output_format`    | Optional output mode: Python, YAML, JSON     |

---

## ğŸ¨ Architecture Diagram

```mermaid
flowchart TD
    A[User Wizard UI] --> B[BeeAI Workflow Engine]
    B --> C[Generate Inference]
    C --> D[Evaluate Inference]
    D --> E{Error?}
    E -- Yes --> F[WatsonX Debug Fix]
    F --> C
    E -- No --> G[Compare + Store Best]
    G --> H{Finished?}
    H -- No --> C
    H -- Yes --> I[Return Best Agent]
    I --> A
```



## ğŸ“Œ Summary of Commands

| Task                         | Command                                      |
|------------------------------|----------------------------------------------|
| Clone repo                   | `git clone`                                  |
| Setup venv                   | `python -m venv venv`                        |
| Install deps                 | `pip install -r requirements.txt`           |
| Launch workflow              | `python -m src.workflows.inference_matcher_workflow` |
| Launch Streamlit debugger    | `streamlit run src/debugger/debugger_app.py` |
| Launch Streamlit wizard      | `streamlit run src/frontend/wizard_app.py`   |
| Run tests                    | `pytest src/tests/`                          |

---

## ğŸ› ï¸ Future Roadmap

- Agent simulation/testing with auto-validation
- Embedding-based output evaluation
- Custom scoring tools for domain-specific agents
- One-click export of generated agents

---

## ğŸ¤ Contributing

All contributions are welcome!

- ğŸ’¡ Propose enhancements
- ğŸ§ª Add test cases
- ğŸ“ Improve documentation
- ğŸ› Fix bugs and open issues

---

## ğŸ“œ License

Licensed under the [MIT License](LICENSE)